{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Project - Rishabh Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps followed in this notebook are to import spark and then load data from a HDFS location into a spark dataframe and then transform data to create a dimensional data model by creating 4 dimensional tables and a fact table, post which we write these tables in a S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing spark and java and cloudera manager\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-7.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter_Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn-client appName=jupyter_Spark>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Checking spark context and whether spark is running fine\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"jupyter_Spark\").setMaster(\"yarn-client\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-7.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter_Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f376c5574d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ETLproject').master(\"local\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have imported spark and tested connection, we'll go ahead and load our data in the required data types before transforming that into specific tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_id', StringType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', DoubleType(),True),\n",
    "                        StructField('atm_lon', DoubleType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True), \n",
    "                        StructField('service', StringType(),True),\n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', DoubleType(),True),\n",
    "                        StructField('weather_lon', DoubleType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True), \n",
    "                        StructField('temp', DoubleType(),True),\n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_degree', IntegerType(),True),\n",
    "                        StructField('rain_3h', DoubleType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True),\n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True),\n",
    "                         \n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Reading file into a dataframe\n",
    "\n",
    "file1 = spark.read.csv(\"/user/root/atm_data/part-m-00000\", header = True, schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: double (nullable = true)\n",
      " |-- atm_lon: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: double (nullable = true)\n",
      " |-- weather_lon: double (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_degree: integer (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Print current schema \n",
    "file1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_degree|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|MasterCard|              1764|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|        250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|      VISA|              1891|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|        250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|     DKK|      VISA|              4166|Withdrawal|        null|        null|     56.139|      9.158|        2619426|            Ikast|281.15|    1011|     100|         6|        240|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|2017|January|  1| Sunday|   0|    Active|     4|             NCR|  Svogerslev|       BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|     DKK|MasterCard|              5153|Withdrawal|        null|        null|     55.642|      12.08|        2614481|         Roskilde|280.61|    1014|      87|         7|        260|    0.0|        88|       701|        Mist|                mist|\n",
      "|2017|January|  1| Sunday|   0|    Active|     5|             NCR|        Nibe|             Torvet|                1|       9240| 56.983|  9.639|     DKK|MasterCard|              3269|Withdrawal|        null|        null|     56.981|      9.639|        2616483|             Nibe|280.64|    1020|      93|         9|        250|   0.59|        92|       500|        Rain|          light rain|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Checking preliminary count of data\n",
    "file1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we find that the count comes to be 1 less than the validation count, this is because we are including Header = true clause while reading the data frame, it is doen specifically beacause it'll help having Headers in future transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And for the sake of simplicity we take the count to be 2468571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'weekday',\n",
       " 'hour',\n",
       " 'atm_status',\n",
       " 'atm_id',\n",
       " 'atm_manufacturer',\n",
       " 'atm_location',\n",
       " 'atm_streetname',\n",
       " 'atm_street_number',\n",
       " 'atm_zipcode',\n",
       " 'atm_lat',\n",
       " 'atm_lon',\n",
       " 'currency',\n",
       " 'card_type',\n",
       " 'transaction_amount',\n",
       " 'service',\n",
       " 'message_code',\n",
       " 'message_text',\n",
       " 'weather_lat',\n",
       " 'weather_lon',\n",
       " 'weather_city_id',\n",
       " 'weather_city_name',\n",
       " 'temp',\n",
       " 'pressure',\n",
       " 'humidity',\n",
       " 'wind_speed',\n",
       " 'wind_degree',\n",
       " 'rain_3h',\n",
       " 'clouds_all',\n",
       " 'weather_id',\n",
       " 'weather_main',\n",
       " 'weather_description']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = file1.select('year','month','day','weekday','hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing functions from pyspark.sql\n",
    "from pyspark.sql import functions as sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we go ahead and start our transformation on the data to create DIM_DATE first and for that we need a time stamp column and then a date_id which is more like a row_number and will work as a primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+---------+\n",
      "|year|  month|day|weekday|hour|month_num|\n",
      "+----+-------+---+-------+----+---------+\n",
      "|2017|January|  1| Sunday|   0|       01|\n",
      "|2017|January|  1| Sunday|   0|       01|\n",
      "|2017|January|  1| Sunday|   0|       01|\n",
      "|2017|January|  1| Sunday|   0|       01|\n",
      "|2017|January|  1| Sunday|   0|       01|\n",
      "+----+-------+---+-------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Converting month names to month numbers to get a datatime field\n",
    "file2 = file2.withColumn(\"month_num\",sf.from_unixtime(sf.unix_timestamp(sf.col(\"month\"),'MMMM'),'MM'))\n",
    "file2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the day, month_num, year, hour to create a datetime field which afterward we'll change to timestamp data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+---------+-----------+\n",
      "|year|  month|day|weekday|hour|month_num|       date|\n",
      "+----+-------+---+-------+----+---------+-----------+\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|\n",
      "+----+-------+---+-------+----+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file2 = file2.withColumn('date', sf.concat(sf.col('day'),sf.lit(' '), sf.col('month_num'),sf.lit(' '), sf.col('year'),sf.lit(' '), sf.col('hour')))\n",
    "file2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[year: int, month: string, day: int, weekday: string, hour: int, month_num: string, date: string]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "|year|  month|day|weekday|hour|month_num|       date|     full_date_time|\n",
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Converting string datetime to timestamp\n",
    "\n",
    "pattern1 = 'dd MM yyyy HH'\n",
    "file3 = file2.withColumn('full_date_time', sf.unix_timestamp(file2['date'], pattern1).cast('timestamp'))\n",
    "file3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking the distinct records in our dataframe for DIM_DATE\n",
    "\n",
    "file3 = file3.distinct()\n",
    "file3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "|year|  month|day|weekday|hour|month_num|       date|     full_date_time|\n",
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "|2017|January|  1| Sunday|   0|       01|1 01 2017 0|2017-01-01 00:00:00|\n",
      "|2017|January|  1| Sunday|   1|       01|1 01 2017 1|2017-01-01 01:00:00|\n",
      "|2017|January|  1| Sunday|   2|       01|1 01 2017 2|2017-01-01 02:00:00|\n",
      "|2017|January|  1| Sunday|   3|       01|1 01 2017 3|2017-01-01 03:00:00|\n",
      "|2017|January|  1| Sunday|   4|       01|1 01 2017 4|2017-01-01 04:00:00|\n",
      "+----+-------+---+-------+----+---------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Order by on the full_date_time column would help us do a date_id and help in transformations\n",
    "\n",
    "file3 = file3.orderBy('full_date_time')\n",
    "file3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now create a surrogate key of row numbers which will act as a primary key. We use monotonically_increasing_id() function to do this and this has been the approach for all the primary key ID columns in all future tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+---------+------------+-------------------+---+-------+\n",
      "|year|  month|day|weekday|hour|month_num|        date|     full_date_time| id|date_id|\n",
      "+----+-------+---+-------+----+---------+------------+-------------------+---+-------+\n",
      "|2017|January|  1| Sunday|   0|       01| 1 01 2017 0|2017-01-01 00:00:00|  0|      1|\n",
      "|2017|January|  1| Sunday|   1|       01| 1 01 2017 1|2017-01-01 01:00:00|  1|      2|\n",
      "|2017|January|  1| Sunday|   2|       01| 1 01 2017 2|2017-01-01 02:00:00|  2|      3|\n",
      "|2017|January|  1| Sunday|   3|       01| 1 01 2017 3|2017-01-01 03:00:00|  3|      4|\n",
      "|2017|January|  1| Sunday|   4|       01| 1 01 2017 4|2017-01-01 04:00:00|  4|      5|\n",
      "|2017|January|  1| Sunday|   5|       01| 1 01 2017 5|2017-01-01 05:00:00|  5|      6|\n",
      "|2017|January|  1| Sunday|   6|       01| 1 01 2017 6|2017-01-01 06:00:00|  6|      7|\n",
      "|2017|January|  1| Sunday|   7|       01| 1 01 2017 7|2017-01-01 07:00:00|  7|      8|\n",
      "|2017|January|  1| Sunday|   8|       01| 1 01 2017 8|2017-01-01 08:00:00|  8|      9|\n",
      "|2017|January|  1| Sunday|   9|       01| 1 01 2017 9|2017-01-01 09:00:00|  9|     10|\n",
      "|2017|January|  1| Sunday|  10|       01|1 01 2017 10|2017-01-01 10:00:00| 10|     11|\n",
      "|2017|January|  1| Sunday|  11|       01|1 01 2017 11|2017-01-01 11:00:00| 11|     12|\n",
      "|2017|January|  1| Sunday|  12|       01|1 01 2017 12|2017-01-01 12:00:00| 12|     13|\n",
      "|2017|January|  1| Sunday|  13|       01|1 01 2017 13|2017-01-01 13:00:00| 13|     14|\n",
      "|2017|January|  1| Sunday|  14|       01|1 01 2017 14|2017-01-01 14:00:00| 14|     15|\n",
      "|2017|January|  1| Sunday|  15|       01|1 01 2017 15|2017-01-01 15:00:00| 15|     16|\n",
      "|2017|January|  1| Sunday|  16|       01|1 01 2017 16|2017-01-01 16:00:00| 16|     17|\n",
      "|2017|January|  1| Sunday|  17|       01|1 01 2017 17|2017-01-01 17:00:00| 17|     18|\n",
      "|2017|January|  1| Sunday|  18|       01|1 01 2017 18|2017-01-01 18:00:00| 18|     19|\n",
      "|2017|January|  1| Sunday|  19|       01|1 01 2017 19|2017-01-01 19:00:00| 19|     20|\n",
      "+----+-------+---+-------+----+---------+------------+-------------------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file3=file3.coalesce(1);\n",
    "file4 = file3.withColumn('id',sf.monotonically_increasing_id())\n",
    "file4 = file4.withColumn('date_id', sf.col('id')+1)\n",
    "file4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+-------+---+----+-------+\n",
      "|date_id|     full_date_time|year|  month|day|hour|weekday|\n",
      "+-------+-------------------+----+-------+---+----+-------+\n",
      "|      1|2017-01-01 00:00:00|2017|January|  1|   0| Sunday|\n",
      "|      2|2017-01-01 01:00:00|2017|January|  1|   1| Sunday|\n",
      "|      3|2017-01-01 02:00:00|2017|January|  1|   2| Sunday|\n",
      "|      4|2017-01-01 03:00:00|2017|January|  1|   3| Sunday|\n",
      "|      5|2017-01-01 04:00:00|2017|January|  1|   4| Sunday|\n",
      "|      6|2017-01-01 05:00:00|2017|January|  1|   5| Sunday|\n",
      "|      7|2017-01-01 06:00:00|2017|January|  1|   6| Sunday|\n",
      "|      8|2017-01-01 07:00:00|2017|January|  1|   7| Sunday|\n",
      "|      9|2017-01-01 08:00:00|2017|January|  1|   8| Sunday|\n",
      "|     10|2017-01-01 09:00:00|2017|January|  1|   9| Sunday|\n",
      "|     11|2017-01-01 10:00:00|2017|January|  1|  10| Sunday|\n",
      "|     12|2017-01-01 11:00:00|2017|January|  1|  11| Sunday|\n",
      "|     13|2017-01-01 12:00:00|2017|January|  1|  12| Sunday|\n",
      "|     14|2017-01-01 13:00:00|2017|January|  1|  13| Sunday|\n",
      "|     15|2017-01-01 14:00:00|2017|January|  1|  14| Sunday|\n",
      "|     16|2017-01-01 15:00:00|2017|January|  1|  15| Sunday|\n",
      "|     17|2017-01-01 16:00:00|2017|January|  1|  16| Sunday|\n",
      "|     18|2017-01-01 17:00:00|2017|January|  1|  17| Sunday|\n",
      "|     19|2017-01-01 18:00:00|2017|January|  1|  18| Sunday|\n",
      "|     20|2017-01-01 19:00:00|2017|January|  1|  19| Sunday|\n",
      "+-------+-------------------+----+-------+---+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Moving the required columsn for DIm_DATE to datdf dataframe\n",
    "datedf = file4.select('date_id','full_date_time','year','month','day','hour','weekday')\n",
    "datedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of records to check with the validations given for the assignment\n",
    "datedf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the count of records for DIM_DATE matches with the number given in the validation doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[date_id: bigint, full_date_time: timestamp, year: int, month: string, day: int, hour: int, weekday: string]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datedf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[date_id: int, full_date_time: timestamp, year: int, month: string, day: int, hour: int, weekday: string]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We relized that dat_id is bigint and wwe want it as integer in our target dimensional model so we change the type to integer\n",
    "datedf = datedf.withColumn(\"date_id\", datedf[\"date_id\"].cast(IntegerType()))\n",
    "datedf.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have done our transformation and created our table, we'll write the table in our S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To write to our S3 location, \\\n",
    "### we need to send the access_key and secret_access_key to spark context so that we can write without access issues \n",
    "\n",
    "#Setting S3 credentials\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "#Read the credentials file\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "\n",
    "#get sccess keys\n",
    "access_id = config.get(\"default\",\"aws_access_key_id\") \n",
    "secret_access_key = config.get(\"default\", \"aws_secret_access_key\")\n",
    "\n",
    "#Set keys into context\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\",  secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedf.write.csv(\"s3n://myetlproject/DIM_DATE\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to DIM_CARD_TYPE table now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           card_type|\n",
      "+--------------------+\n",
      "|     Dankort - on-us|\n",
      "|              CIRRUS|\n",
      "|         HÃƒÂ¦vekort|\n",
      "|                VISA|\n",
      "|  Mastercard - on-us|\n",
      "|             Maestro|\n",
      "|Visa Dankort - on-us|\n",
      "|        Visa Dankort|\n",
      "|            VisaPlus|\n",
      "|          MasterCard|\n",
      "|             Dankort|\n",
      "| HÃƒÂ¦vekort - on-us|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file10  = file1.select('card_type').distinct()\n",
    "file10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+\n",
      "|           card_type| id|card_type_id|\n",
      "+--------------------+---+------------+\n",
      "|     Dankort - on-us|  0|           1|\n",
      "|              CIRRUS|  1|           2|\n",
      "|         HÃƒÂ¦vekort|  2|           3|\n",
      "|                VISA|  3|           4|\n",
      "|  Mastercard - on-us|  4|           5|\n",
      "|             Maestro|  5|           6|\n",
      "|Visa Dankort - on-us|  6|           7|\n",
      "|        Visa Dankort|  7|           8|\n",
      "|            VisaPlus|  8|           9|\n",
      "|          MasterCard|  9|          10|\n",
      "|             Dankort| 10|          11|\n",
      "| HÃƒÂ¦vekort - on-us| 11|          12|\n",
      "+--------------------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating ID field as primary key using the same method\n",
    "\n",
    "file10=file10.coalesce(1);\n",
    "file11 = file10.withColumn('id',sf.monotonically_increasing_id())\n",
    "file11 = file11.withColumn('card_type_id', sf.col('id')+1)\n",
    "file11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[card_type: string, id: bigint, card_type_id: bigint]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file11.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[card_type: string, id: bigint, card_type_id: int]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Converting to Integer type from big int\n",
    "file11 = file11.withColumn(\"card_type_id\", file11[\"card_type_id\"].cast(IntegerType()))\n",
    "file11.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|         HÃƒÂ¦vekort|\n",
      "|           4|                VISA|\n",
      "|           5|  Mastercard - on-us|\n",
      "|           6|             Maestro|\n",
      "|           7|Visa Dankort - on-us|\n",
      "|           8|        Visa Dankort|\n",
      "|           9|            VisaPlus|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12| HÃƒÂ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "card_type_df = file11.select('card_type_id','card_type')\n",
    "card_type_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking count for validation \n",
    "card_type_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice that validation count matches, so far so good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_df.write.csv(\"s3n://myetlproject/DIM_CARD_TYPE\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to DIM_LOCATION table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|        atm_location|  atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|               Vadum|  Ellehammersvej|               43|       9430| 57.118|  9.861|\n",
      "|            Slagelse| Mariendals Alle|               29|       4200| 55.398| 11.342|\n",
      "|          Fredericia|SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|\n",
      "|             Kolding|        Vejlevej|              135|       6000| 55.505|  9.457|\n",
      "|   HÃƒÂ¸rning Hallen|        Toftevej|               53|       8362| 56.091| 10.033|\n",
      "|                Aars| Himmerlandsgade|               70|       9600| 56.803|  9.518|\n",
      "|                 Fur|      StenÃƒÂ¸re|               19|       7884| 56.805|   9.02|\n",
      "|     Aarhus Lufthavn| Ny Lufthavnsvej|               24|       8560| 56.308| 10.627|\n",
      "|            Hasseris|     Hasserisvej|              113|       9000| 57.044|  9.898|\n",
      "|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|               75|       1550| 55.676| 12.571|\n",
      "|    Intern HolbÃƒÂ¦k|     Slotsvolden|                7|       4300| 55.718| 11.704|\n",
      "|      Skelagervej 15|     Skelagervej|               15|       9000| 57.023|  9.891|\n",
      "|              Viborg|       Toldboden|                3|       8800| 56.448|  9.401|\n",
      "|             SÃƒÂ¦by|      Vestergade|                3|       9300| 57.334| 10.515|\n",
      "|             Aabybro|    ÃƒËœstergade|                6|       9440| 57.162|   9.73|\n",
      "|             Vodskov|      Vodskovvej|               27|       9310| 57.104| 10.027|\n",
      "|               Taars|        Bredgade|               91|       9830| 57.385| 10.116|\n",
      "|         Skive Lobby|        Adelgade|                8|       7800| 56.567|  9.027|\n",
      "|        HelsingÃƒÂ¸r|  Sct. Olai Gade|               39|       3000| 56.036| 12.612|\n",
      "|             Jebjerg|       Kirkegade|                4|       7870| 56.671|  9.013|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file12 = file1.select('atm_location', 'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon',)\n",
    "file12 = file12.distinct()\n",
    "file12.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Renaming column names based on target dimensional table\n",
    "\n",
    "file12 = file12.withColumnRenamed(\"atm_location\", \"location\")\\\n",
    "        .withColumnRenamed(\"atm_streetname\", \"streetname\")\\\n",
    "        .withColumnRenamed(\"atm_street_number\", \"street_number\")\\\n",
    "        .withColumnRenamed(\"atm_zipcode\", \"zipcode\")\\\n",
    "        .withColumnRenamed(\"atm_lat\", \"lat\")\\\n",
    "        .withColumnRenamed(\"atm_lon\", \"lon\")\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-------------+-------+------+------+\n",
      "|         location|      streetname|street_number|zipcode|   lat|   lon|\n",
      "+-----------------+----------------+-------------+-------+------+------+\n",
      "|            Vadum|  Ellehammersvej|           43|   9430|57.118| 9.861|\n",
      "|         Slagelse| Mariendals Alle|           29|   4200|55.398|11.342|\n",
      "|       Fredericia|SjÃƒÂ¦llandsgade|           33|   7000|55.564| 9.757|\n",
      "|          Kolding|        Vejlevej|          135|   6000|55.505| 9.457|\n",
      "|HÃƒÂ¸rning Hallen|        Toftevej|           53|   8362|56.091|10.033|\n",
      "+-----------------+----------------+-------------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file12.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+-------+------+------+---+-----------+\n",
      "|            location|      streetname|street_number|zipcode|   lat|   lon| id|location_id|\n",
      "+--------------------+----------------+-------------+-------+------+------+---+-----------+\n",
      "|               Vadum|  Ellehammersvej|           43|   9430|57.118| 9.861|  0|          1|\n",
      "|            Slagelse| Mariendals Alle|           29|   4200|55.398|11.342|  1|          2|\n",
      "|          Fredericia|SjÃƒÂ¦llandsgade|           33|   7000|55.564| 9.757|  2|          3|\n",
      "|             Kolding|        Vejlevej|          135|   6000|55.505| 9.457|  3|          4|\n",
      "|   HÃƒÂ¸rning Hallen|        Toftevej|           53|   8362|56.091|10.033|  4|          5|\n",
      "|                Aars| Himmerlandsgade|           70|   9600|56.803| 9.518|  5|          6|\n",
      "|     Aarhus Lufthavn| Ny Lufthavnsvej|           24|   8560|56.308|10.627|  6|          7|\n",
      "|                 Fur|      StenÃƒÂ¸re|           19|   7884|56.805|  9.02|  7|          8|\n",
      "|            Hasseris|     Hasserisvej|          113|   9000|57.044| 9.898|  8|          9|\n",
      "|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|           75|   1550|55.676|12.571|  9|         10|\n",
      "|      Skelagervej 15|     Skelagervej|           15|   9000|57.023| 9.891| 10|         11|\n",
      "|    Intern HolbÃƒÂ¦k|     Slotsvolden|            7|   4300|55.718|11.704| 11|         12|\n",
      "|              Viborg|       Toldboden|            3|   8800|56.448| 9.401| 12|         13|\n",
      "|             SÃƒÂ¦by|      Vestergade|            3|   9300|57.334|10.515| 13|         14|\n",
      "|             Aabybro|    ÃƒËœstergade|            6|   9440|57.162|  9.73| 14|         15|\n",
      "|             Vodskov|      Vodskovvej|           27|   9310|57.104|10.027| 15|         16|\n",
      "|               Taars|        Bredgade|           91|   9830|57.385|10.116| 16|         17|\n",
      "|         Skive Lobby|        Adelgade|            8|   7800|56.567| 9.027| 17|         18|\n",
      "|        HelsingÃƒÂ¸r|  Sct. Olai Gade|           39|   3000|56.036|12.612| 18|         19|\n",
      "|             Jebjerg|       Kirkegade|            4|   7870|56.671| 9.013| 19|         20|\n",
      "+--------------------+----------------+-------------+-------+------+------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Creating ID column\n",
    "file12=file12.coalesce(1);\n",
    "file13 = file12.withColumn('id',sf.monotonically_increasing_id())\n",
    "file13 = file13.withColumn('location_id', sf.col('id')+1)\n",
    "file13.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[location: string, streetname: string, street_number: int, zipcode: int, lat: double, lon: double, id: bigint, location_id: int]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file13 = file13.withColumn(\"location_id\", file13[\"location_id\"].cast(IntegerType()))\n",
    "file13.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------------+-------------+-------+------+------+\n",
      "|location_id|         location|      streetname|street_number|zipcode|   lat|   lon|\n",
      "+-----------+-----------------+----------------+-------------+-------+------+------+\n",
      "|          1|            Vadum|  Ellehammersvej|           43|   9430|57.118| 9.861|\n",
      "|          2|         Slagelse| Mariendals Alle|           29|   4200|55.398|11.342|\n",
      "|          3|       Fredericia|SjÃƒÂ¦llandsgade|           33|   7000|55.564| 9.757|\n",
      "|          4|          Kolding|        Vejlevej|          135|   6000|55.505| 9.457|\n",
      "|          5|HÃƒÂ¸rning Hallen|        Toftevej|           53|   8362|56.091|10.033|\n",
      "+-----------+-----------------+----------------+-------------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_df = file13.select('location_id','location','streetname','street_number','zipcode','lat','lon')\n",
    "location_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Checking final count of the records\n",
    "\n",
    "location_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our validation count matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df.write.csv(\"s3n://myetlproject/DIM_LOCATION\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to DIM_ATM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.select('atm_id','atm_manufacturer','atm_lat','atm_lon').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-------+-------+-----------+-----------+----------+-------------+-------+------+-----+\n",
      "|atm_id|atm_manufacturer|atm_lat|atm_lon|location_id|   location|streetname|street_number|zipcode|   lat|  lon|\n",
      "+------+----------------+-------+-------+-----------+-----------+----------+-------------+-------+------+-----+\n",
      "|   109| Diebold Nixdorf| 57.005|  9.881|         35|Aalborg Syd|  Hobrovej|          440|   9200|57.005|9.881|\n",
      "|   109| Diebold Nixdorf| 57.005|  9.881|         35|Aalborg Syd|  Hobrovej|          440|   9200|57.005|9.881|\n",
      "|   109| Diebold Nixdorf| 57.005|  9.881|         35|Aalborg Syd|  Hobrovej|          440|   9200|57.005|9.881|\n",
      "|   109| Diebold Nixdorf| 57.005|  9.881|         35|Aalborg Syd|  Hobrovej|          440|   9200|57.005|9.881|\n",
      "|   109| Diebold Nixdorf| 57.005|  9.881|         35|Aalborg Syd|  Hobrovej|          440|   9200|57.005|9.881|\n",
      "+------+----------------+-------+-------+-----------+-----------+----------+-------------+-------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Joining with location table to get location ID into atm table\n",
    "\n",
    "atm_f = file1.select('atm_id','atm_manufacturer','atm_lat','atm_lon')\n",
    "left_join = atm_f.join(location_df, (atm_f.atm_lat == location_df.lat) & (atm_f.atm_lon == location_df.lon) ,how='left') # Could also use 'left_outer'\n",
    "left_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atm_f3 = left_join.select('atm_id','atm_manufacturer','location_id')\n",
    "atm_f3.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+\n",
      "|atm_id|atm_manufacturer|location_id|\n",
      "+------+----------------+-----------+\n",
      "|     6|             NCR|          3|\n",
      "|   104|             NCR|         25|\n",
      "|    62| Diebold Nixdorf|         30|\n",
      "|    45|             NCR|         22|\n",
      "|    95|             NCR|         10|\n",
      "+------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atm_f3 = atm_f3.distinct()\n",
    "atm_f3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+\n",
      "|atm_number|atm_manufacturer|atm_location_id|\n",
      "+----------+----------------+---------------+\n",
      "|       104|             NCR|             25|\n",
      "|         6|             NCR|              3|\n",
      "|        95|             NCR|             10|\n",
      "|        62| Diebold Nixdorf|             30|\n",
      "|        25| Diebold Nixdorf|             38|\n",
      "+----------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Renaming atm table names based on target schema\n",
    "\n",
    "atm_f3 = atm_f3.withColumnRenamed(\"atm_id\", \"atm_number\")\\\n",
    "        .withColumnRenamed(\"location_id\", \"atm_location_id\")\n",
    "atm_f3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+---+------+\n",
      "|atm_number|atm_manufacturer|atm_location_id| id|atm_id|\n",
      "+----------+----------------+---------------+---+------+\n",
      "|         6|             NCR|              3|  0|     1|\n",
      "|       104|             NCR|             25|  1|     2|\n",
      "|        62| Diebold Nixdorf|             30|  2|     3|\n",
      "|        95|             NCR|             10|  3|     4|\n",
      "|        25| Diebold Nixdorf|             38|  4|     5|\n",
      "|        25| Diebold Nixdorf|             94|  5|     6|\n",
      "|        40| Diebold Nixdorf|             88|  6|     7|\n",
      "|        51|             NCR|             65|  7|     8|\n",
      "|        56| Diebold Nixdorf|             61|  8|     9|\n",
      "|        16|             NCR|             34|  9|    10|\n",
      "|        91|             NCR|             18| 10|    11|\n",
      "|       110| Diebold Nixdorf|             37| 11|    12|\n",
      "|         9| Diebold Nixdorf|             77| 12|    13|\n",
      "|        82|             NCR|             79| 13|    14|\n",
      "|        97|             NCR|             46| 14|    15|\n",
      "|        26|             NCR|             37| 15|    16|\n",
      "|        55|             NCR|             39| 16|    17|\n",
      "|       102|             NCR|             93| 17|    18|\n",
      "|       108|             NCR|              5| 18|    19|\n",
      "|       100|             NCR|             70| 19|    20|\n",
      "+----------+----------------+---------------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Including ID column for the primary key\n",
    "\n",
    "atm_f3=atm_f3.coalesce(1);\n",
    "atm_f4 = atm_f3.withColumn('id',sf.monotonically_increasing_id())\n",
    "atm_f4 = atm_f4.withColumn('atm_id', sf.col('id')+1)\n",
    "atm_f4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location_id: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      " |-- atm_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atm_f4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+---------------+\n",
      "|atm_id|atm_number|atm_manufacturer|atm_location_id|\n",
      "+------+----------+----------------+---------------+\n",
      "|     1|       104|             NCR|             25|\n",
      "|     2|         6|             NCR|              3|\n",
      "|     3|        62| Diebold Nixdorf|             30|\n",
      "|     4|        95|             NCR|             10|\n",
      "|     5|        25| Diebold Nixdorf|             38|\n",
      "+------+----------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atm_df = atm_f4.select('atm_id','atm_number','atm_manufacturer','atm_location_id')\n",
    "atm_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_id: integer (nullable = false)\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atm_df = atm_df.withColumn(\"atm_id\", atm_df[\"atm_id\"].cast(IntegerType()))\n",
    "atm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Checking count for validation purposes\n",
    "atm_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation check for this table passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_df.write.csv(\"s3n://myetlproject/DIM_ATM\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to FACT_ATM_TRANS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to do a couple of joins here to get foreign key fields like card_type_id, date_id, weather_loc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting important columns\n",
    "\n",
    "fact_f = file1.select('atm_status','currency','service','transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description','atm_id','atm_location', \\\n",
    "                      'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon','card_type','year','month','day','hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468571"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Getting card_type_id and then checking count to maintain the count of the table\n",
    "fact_f_join = fact_f.join(card_type_df, fact_f.card_type == card_type_df.card_type , how = 'left') \n",
    "                            \n",
    "fact_f_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_f = fact_f_join.select('atm_status','currency','service','transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description','atm_id','atm_location', \\\n",
    "                      'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon','card_type_id','year','month','day','hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468571"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Joining with DIM_DATE to get date_id\n",
    "fact_f_join10 = fact_f.join(datedf, (fact_f.year == datedf.year) & (fact_f.month == datedf.month) & \\\n",
    "                            (fact_f.day == datedf.day) & (fact_f.hour == datedf.hour), how = 'left') \n",
    "\n",
    "fact_f_join10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_f = fact_f_join10.select('atm_status','currency','service','transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description','atm_id','atm_location', \\\n",
    "                      'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon','card_type_id','date_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468571"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Join with location table to get location Id which will be weather_loc_id\n",
    "\n",
    "fact_f_join1 = fact_f.join(location_df, (fact_f.atm_lon  == location_df.lon) & \\\n",
    "                           (fact_f.atm_lat == location_df.lat) & (fact_f.atm_location == location_df.location) &\\\n",
    "                           (fact_f.atm_streetname == location_df.streetname) & (fact_f.atm_street_number == location_df.street_number) & \\\n",
    "                           (fact_f.atm_zipcode == location_df.zipcode) , how = 'left') \n",
    "                            \n",
    "fact_f_join1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choosing appropriate columns required for target dataframe\n",
    "\n",
    "fact_f_join1 = fact_f_join1.select('atm_status','currency','service','transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description','atm_id','location_id', 'card_type_id','date_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming fields\n",
    "\n",
    "fact_f_join1 = fact_f_join1.withColumnRenamed(\"atm_id\", \"atm_num\")\\\n",
    "        .withColumnRenamed(\"location_id\", \"weather_loc_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468571"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Joining with DIM_ATM to get ATM_ID\n",
    "\n",
    "fact_f_join2 = fact_f_join1.join(atm_df, (fact_f_join1.atm_num == atm_df.atm_number) & \\\n",
    "                                 (fact_f_join1.weather_loc_id == atm_df.atm_location_id), how = 'left') \n",
    "                            \n",
    "fact_f_join2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_f_join2 = fact_f_join2.select ('atm_status','currency','service','transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description','atm_number','weather_loc_id','atm_id','card_type_id','date_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tran_df  = fact_f_join2.select('atm_id','weather_loc_id','date_id','card_type_id','atm_status','currency','service',\\\n",
    "                                    'transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating a primary key TRANS_ID\n",
    "\n",
    "fact_tran_df=fact_tran_df.coalesce(1);\n",
    "fact_atm_tran_df = fact_tran_df.withColumn('id',sf.monotonically_increasing_id())\n",
    "fact_atm_tran_df = fact_atm_tran_df.withColumn('trans_id', sf.col('id')+1)\n",
    "fact_atm_tran_df = fact_atm_tran_df.select('trans_id','atm_id','weather_loc_id','date_id','card_type_id','atm_status','currency','service',\\\n",
    "                                    'transaction_amount','message_code','message_text', 'rain_3h',\\\n",
    "                      'clouds_all','weather_id','weather_main','weather_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_tran_df1 = fact_atm_tran_df.select('trans_id',  'atm_id', 'weather_loc_id','date_id' ,'card_type_id','atm_status',\n",
    "                                            'clouds_all', 'weather_id','weather_main','weather_description','transaction_amount',\n",
    "                                           'message_code', 'message_text', 'currency','service',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_atm_tran_df1.write.csv(\"s3n://myetlproject/FACT_ATM_TYPE\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the count is matching the validation is also passed and we have written the tables into 5 different folders in the S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
